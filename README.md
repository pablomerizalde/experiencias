# ğŸ¤– Chatbot GenAI - Caso de Estudio Recursos Humanos

Este proyecto demuestra cÃ³mo construir, evaluar y automatizar un chatbot de tipo RAG (Retrieval Augmented Generation) con buenas prÃ¡cticas de **GenAIOps**.

---

## ğŸ§  Caso de Estudio

El chatbot responde preguntas sobre beneficios, polÃ­ticas internas y roles de una empresa ficticia (**Contoso Electronics**), usando como base una colecciÃ³n de documentos PDF internos.

---

## ğŸ“‚ Estructura del Proyecto

```
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ ui_streamlit.py           â† interfaz simple del chatbot
â”‚   â”œâ”€â”€ main_interface.py         â† interfaz combinada con mÃ©tricas
â”‚   â”œâ”€â”€ run_eval.py               â† evaluaciÃ³n automÃ¡tica
â”‚   â”œâ”€â”€ rag_pipeline.py           â† lÃ³gica de ingestiÃ³n y RAG
â”‚   â””â”€â”€ prompts/
â”‚       â”œâ”€â”€ v1_asistente_rrhh.txt
â”‚       â””â”€â”€ v2_resumido_directo.txt
â”œâ”€â”€ data/pdfs/                    â† documentos fuente
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_run_eval.py
â”‚   â”œâ”€â”€ eval_dataset.json         â† dataset de evaluaciÃ³n
â”‚   â””â”€â”€ eval_dataset.csv
â”œâ”€â”€ .env.example
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ .devcontainer/
â”‚   â””â”€â”€ devcontainer.json
â”œâ”€â”€ .github/workflows/
â”‚   â”œâ”€â”€ eval.yml
â”‚   â””â”€â”€ test.yml
```

---

## ğŸš¦ Ciclo de vida GenAIOps aplicado

### 1. ğŸ§± PreparaciÃ³n del entorno

```bash
git clone <repositorio>
cd chatbot-genaiops
conda create -n chatbot-genaiops python=3.10 -y
conda activate chatbot-genaiops
pip install -r requirements.txt
cp .env.example .env  # Agrega tu API KEY de OpenAI
```

---

### 2. ğŸ” Ingesta y vectorizaciÃ³n de documentos

Procesa los PDFs y genera el Ã­ndice vectorial:

```bash
python -c "from app.rag_pipeline import save_vectorstore; save_vectorstore()"
```

Esto:
- Divide los documentos en chunks (por defecto `chunk_size=512`, `chunk_overlap=50`)
- Genera embeddings con OpenAI
- Guarda el Ã­ndice vectorial en `vectorstore/`
- Registra los parÃ¡metros en **MLflow**

ğŸ”§ Para personalizar:
```python
save_vectorstore(chunk_size=1024, chunk_overlap=100)
```

â™»ï¸ Para reutilizarlo directamente:
```python
vectordb = load_vectorstore_from_disk()
```

---

### 3. ğŸ§  ConstrucciÃ³n del pipeline RAG

```python
from app.rag_pipeline import build_chain
chain = build_chain(vectordb, prompt_version="v1_asistente_rrhh")
```

- Soporta mÃºltiples versiones de prompt
- Usa `ConversationalRetrievalChain` con `LangChain` + `OpenAI`

---

### 4. ğŸ’¬ InteracciÃ³n vÃ­a Streamlit

VersiÃ³n bÃ¡sica:
```bash
streamlit run app/ui_streamlit.py
```

VersiÃ³n combinada con mÃ©tricas:
```bash
streamlit run app/main_interface.py
```

---

### 5. ğŸ§ª EvaluaciÃ³n automÃ¡tica de calidad

Ejecuta:

```bash
python app/run_eval.py
```

Esto:
- Usa `tests/eval_dataset.json` como ground truth
- Genera respuestas usando el RAG actual
- EvalÃºa con `LangChain Eval (QAEvalChain)`
- Registra resultados en **MLflow**

---

### 6. ğŸ“ˆ VisualizaciÃ³n de resultados

Dashboard completo:

```bash
streamlit run app/dashboard_all.py
```

- Tabla con todas las preguntas evaluadas
- GrÃ¡ficos de precisiÃ³n por configuraciÃ³n (`prompt + chunk_size`)
- Filtrado por experimento MLflow

---

### 7. ğŸ” AutomatizaciÃ³n con GitHub Actions

- CI de evaluaciÃ³n: `.github/workflows/eval.yml`
- Test unitarios: `.github/workflows/test.yml`

---

### 8. ğŸ§ª ValidaciÃ³n automatizada

```bash
pytest tests/test_run_eval.py
```

- EvalÃºa que el sistema tenga al menos 80% de precisiÃ³n con el dataset base

---

## ğŸ” Â¿QuÃ© puedes hacer?

- ğŸ’¬ Hacer preguntas al chatbot
- ğŸ” Evaluar diferentes estrategias de chunking y prompts
- ğŸ“Š Comparar desempeÃ±o con mÃ©tricas semÃ¡nticas
- ğŸ§ª Trazar todo en MLflow
- ğŸ”„ Adaptar a otros dominios (legal, salud, educaciÃ³nâ€¦)

---

## âš™ï¸ Stack TecnolÃ³gico

- **OpenAI + LangChain** â€“ LLM + RAG
- **FAISS** â€“ Vectorstore
- **Streamlit** â€“ UI
- **MLflow** â€“ Registro de experimentos
- **LangChain Eval** â€“ EvaluaciÃ³n semÃ¡ntica
- **GitHub Actions** â€“ CI/CD
- **DevContainer** â€“ Desarrollo portable

---

## ğŸ“ DesafÃ­o para estudiantes

ğŸ§© Parte 1: PersonalizaciÃ³n

1. Elige un nuevo dominio
Ejemplos: salud, educaciÃ³n, legal, bancario, etc.

2. Reemplaza los documentos PDF
UbÃ­calos en data/pdfs/.

3. Modifica o crea tus prompts
Edita los archivos en app/prompts/.

4. Crea un conjunto de pruebas
En tests/eval_dataset.json, define preguntas y respuestas esperadas para evaluar a tu chatbot.

âœ… Parte 2: EvaluaciÃ³n AutomÃ¡tica

1. Ejecuta run_eval.py para probar tu sistema actual.
Actualmente, la evaluaciÃ³n estÃ¡ basada en QAEvalChain de LangChain, que devuelve una mÃ©trica binaria: correcto / incorrecto.

ğŸ”§ Parte 3: Â¡Tu reto! (ğŸ‘¨â€ğŸ”¬ nivel investigador)

1. Mejora el sistema de evaluaciÃ³n:

    * Agrega evaluaciÃ³n con LabeledCriteriaEvalChain usando al menos los siguientes criterios:

        * "correctness" â€“ Â¿Es correcta la respuesta?
        * "relevance" â€“ Â¿Es relevante respecto a la pregunta?
        * "coherence" â€“ Â¿EstÃ¡ bien estructurada la respuesta?
        * "toxicity" â€“ Â¿Contiene lenguaje ofensivo o riesgoso?
        * "harmfulness" â€“ Â¿PodrÃ­a causar daÃ±o la informaciÃ³n?

    * Cada criterio debe registrar:

        * Una mÃ©trica en MLflow (score)

    * Y opcionalmente, un razonamiento como artefacto (reasoning)

    ğŸ“š Revisa la [documentaciÃ³n de LabeledCriteriaEvalChain](https://python.langchain.com/api_reference/langchain/evaluation/langchain.evaluation.criteria.eval_chain.LabeledCriteriaEvalChain.html) para implementarlo.

ğŸ“Š Parte 4: Mejora el dashboard

1. Extiende dashboard.py o main_interface.py para visualizar:

    * Las mÃ©tricas por criterio (correctness_score, toxicity_score, etc.).
    * Una opciÃ³n para seleccionar y comparar diferentes criterios en grÃ¡ficos.
    * (Opcional) Razonamientos del modelo como texto.    

ğŸ§ª Parte 5: Presenta y reflexiona
1. Compara configuraciones distintas (chunk size, prompt) y justifica tu selecciÃ³n.
    * Â¿CuÃ¡l configuraciÃ³n genera mejores respuestas?
    * Â¿En quÃ© fallan los modelos? Â¿Fueron tÃ³xicos o incoherentes?
    * Usa evidencias desde MLflow y capturas del dashboard.

ğŸš€ Bonus

- Â¿Te animas a crear un nuevo criterio como "claridad" o "creatividad"? Puedes definirlo tÃº mismo y usarlo con LabeledCriteriaEvalChain.

---

Â¡Listo para ser usado en clase, investigaciÃ³n o producciÃ³n educativa! ğŸš€
